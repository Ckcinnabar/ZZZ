# CNN+LSTM 語音情緒識別系統完整技術說明

## 📋 目錄
1. [系統概述](#1-系統概述)
2. [整體架構流程](#2-整體架構流程)
3. [數據預處理](#3-數據預處理)
4. [特徵提取：Mel頻譜圖](#4-特徵提取mel頻譜圖)
5. [模型架構詳解](#5-模型架構詳解)
6. [CNN卷積層：空間特徵提取](#6-cnn卷積層空間特徵提取)
7. [LSTM層：時序特徵建模](#7-lstm層時序特徵建模)
8. [全連接層：情緒分類](#8-全連接層情緒分類)
9. [數據增強技術](#9-數據增強技術)
10. [訓練策略](#10-訓練策略)
11. [模型性能分析](#11-模型性能分析)
12. [完整數據流示例](#12-完整數據流示例)

---

## 1. 系統概述

### 1.1 問題定義
**任務**：從語音信號中識別說話者的情緒狀態
**輸入**：原始音頻波形（144,000個採樣點，48kHz採樣率，約3秒）
**輸出**：5種情緒類別之一（Emotion 1-5）
**準確率**：測試集達到 **87.41%**

### 1.2 為什麼使用 CNN+LSTM 混合架構？

| 組件 | 作用 | 處理的信息類型 |
|------|------|----------------|
| **CNN** | 提取頻譜圖中的空間模式 | 頻率特徵、音色、音高 |
| **LSTM** | 捕捉時間序列中的動態變化 | 語調變化、韻律、情緒演變 |
| **組合效果** | 同時理解「說什麼」和「怎麼說」 | 完整的情緒表達 |

**核心理念**：情緒不僅體現在聲音的頻率特徵（音色、音高），還體現在這些特徵隨時間的變化（語調起伏、節奏快慢）。

---

## 2. 整體架構流程

```
原始音頻波形 (900 samples × 144,000 points)
        ↓
[1. 特徵提取] Mel頻譜圖轉換
        ↓
Mel頻譜圖 (900 × 128頻帶 × 282時間幀)
        ↓
[2. 數據增強] 噪聲、時移、音量調整 (5x擴增)
        ↓
增強數據 (3,150 訓練樣本)
        ↓
[3. CNN特徵提取] 3層卷積 + 池化
        ↓
空間特徵圖 (batch × 128通道 × 16高 × 35寬)
        ↓
[4. 重塑為序列] (batch × 35時間步 × 2048特徵)
        ↓
[5. LSTM時序建模] 雙向2層LSTM
        ↓
時序編碼 (batch × 256)
        ↓
[6. 全連接分類] FC層 + Dropout
        ↓
情緒預測 (5類概率分佈)
```

---

## 3. 數據預處理

### 3.1 原始數據特性
```python
train_X.shape = (900, 144000)  # 900個樣本，每個144,000採樣點
train_t.shape = (900,)         # 對應的情緒標籤
```

**物理含義**：
- **採樣率**：48,000 Hz（每秒48,000個採樣點）
- **時長**：144,000 / 48,000 = **3秒**
- **類別分佈**：5類情緒，每類約180個樣本（平衡數據集）

### 3.2 數據劃分
```python
訓練集：70% (630 samples)
驗證集：15% (135 samples)
測試集：15% (135 samples)
```

**分層抽樣**：確保每個集合都包含所有情緒類別的均衡樣本。

---

## 4. 特徵提取：Mel頻譜圖

### 4.1 為什麼不直接使用原始波形？

| 表示方式 | 優點 | 缺點 |
|----------|------|------|
| **原始波形** | 保留完整信息 | 144,000維太高，難以訓練 |
| **Mel頻譜圖** | 降維至128×282，符合人耳感知 | 有損壓縮 |

### 4.2 Mel頻譜圖生成過程

```python
melspec = librosa.feature.melspectrogram(
    y=train_X,
    sr=48000,           # 採樣率
    n_fft=1024,         # FFT窗口大小
    hop_length=512,     # 窗口滑動步長
    n_mels=128,         # Mel頻帶數量
    fmax=8000           # 最大頻率 8kHz
)
```

**步驟詳解**：

#### Step 1: 短時傅立葉變換 (STFT)
```
原始波形 → 滑動窗口 (1024點) → FFT → 頻譜
```
- **窗口大小**：1024點 ≈ 21ms (1024/48000)
- **滑動步長**：512點 ≈ 10.7ms（50%重疊）
- **時間幀數**：144,000 / 512 ≈ 282幀

#### Step 2: Mel濾波器組
```
線性頻率 → Mel刻度轉換 → 128個Mel頻帶
```
**Mel刻度**：模擬人耳對頻率的非線性感知
- 低頻（<1000Hz）：更多頻帶（分辨率高）
- 高頻（>1000Hz）：較少頻帶（分辨率低）

#### Step 3: 功率轉對數刻度 (dB)
```python
melspec_db = librosa.power_to_db(melspec, ref=np.max)
```
**原因**：人耳對聲音強度的感知是對數的（分貝刻度）

### 4.3 最終特徵形狀
```
輸入： (900, 144000)         # 原始波形
輸出： (900, 128, 282)       # Mel頻譜圖
      ↓      ↓     ↓
   樣本數  頻率維  時間維
```

**解讀**：
- **128頻帶**：從低音到高音的頻率分佈
- **282時間幀**：3秒音頻的時間演變
- **數值範圍**：-80 到 0 dB（對數能量）

### 4.4 視覺化理解

想像一張熱力圖：
- **橫軸**：時間（左→右 = 開始→結束）
- **縱軸**：頻率（下→上 = 低音→高音）
- **顏色**：能量強度（暗→亮 = 弱→強）

**情緒特徵示例**：
- **憤怒**：高頻能量強，時間變化劇烈
- **悲傷**：低頻為主，能量分佈平緩
- **快樂**：頻率範圍廣，節奏明快

---

## 5. 模型架構詳解

### 5.1 整體結構圖

```
輸入：(batch, 1, 128, 282)
         ↓
    [CNN Block 1]
    Conv2d(1→32) + BN + ReLU + MaxPool
         ↓
    (batch, 32, 64, 141)
         ↓
    [CNN Block 2]
    Conv2d(32→64) + BN + ReLU + MaxPool
         ↓
    (batch, 64, 32, 70)
         ↓
    [CNN Block 3]
    Conv2d(64→128) + BN + ReLU + MaxPool
         ↓
    (batch, 128, 16, 35)
         ↓
    Dropout(0.3)
         ↓
    [重塑為序列]
    (batch, 35, 2048)
         ↓
    [Bidirectional LSTM × 2]
    (batch, 35, 256)
         ↓
    取最後時間步 → (batch, 256)
         ↓
    [全連接層]
    FC(256→128) + ReLU + Dropout
         ↓
    FC(128→5)
         ↓
    輸出：(batch, 5) 情緒概率
```

### 5.2 參數統計
```
總參數：2,752,197
可訓練參數：2,752,197
模型大小：約 10.5 MB
```

---

## 6. CNN卷積層：空間特徵提取

### 6.1 CNN的作用
CNN負責從Mel頻譜圖中提取**局部模式**，例如：
- 特定頻率範圍的能量峰值
- 頻率軌跡（formants）
- 時頻圖案（harmonics）

### 6.2 第一層卷積（Conv1）

```python
Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1)
BatchNorm2d(32)
ReLU()
MaxPool2d(kernel_size=(2,2))
```

**工作原理**：

#### 卷積操作
```
輸入：(batch, 1, 128, 282)
卷積核：32個 (3×3) 濾波器

每個濾波器掃描整個頻譜圖：
┌─────────┐
│ w1 w2 w3│  ← 3×3卷積核
│ w4 w5 w6│
│ w7 w8 w9│
└─────────┘
   ↓ 滑動掃描
[計算局部加權和] → 檢測特定模式

輸出：32個特徵圖 (batch, 32, 128, 282)
```

**學習到的模式示例**：
- **濾波器1**：可能檢測「低頻能量上升」
- **濾波器2**：可能檢測「高頻諧波結構」
- **濾波器32**：可能檢測「頻率跳變」

#### 批歸一化 (BatchNorm)
```python
作用：穩定訓練，加速收斂
操作：將每個特徵圖標準化為均值0，方差1
```

#### 激活函數 (ReLU)
```python
ReLU(x) = max(0, x)
作用：引入非線性，只保留正值激活
```

#### 最大池化 (MaxPool)
```python
kernel_size=(2,2)
作用：降採樣，提取最顯著特徵

示例：
[1  3]  → 取最大值 → [5]
[2  5]

輸出：(batch, 32, 64, 141)
尺寸減半，保留關鍵信息
```

### 6.3 第二層卷積（Conv2）

```python
Conv2d(32 → 64) + BN + ReLU + MaxPool
```

**變化**：
- 通道數加倍（32→64）：學習更複雜的組合特徵
- 空間尺寸再減半（64×141 → 32×70）

**學習到的特徵**：
- 組合前一層的簡單模式
- 例如：「低頻能量上升」+「高頻諧波」→「憤怒的聲調模式」

### 6.4 第三層卷積（Conv3）

```python
Conv2d(64 → 128) + BN + ReLU + MaxPool
```

**最終輸出**：
```
(batch, 128, 16, 35)
   ↓     ↓    ↓  ↓
 批次  通道  頻率 時間
```

**高級特徵**：
- 128個高級特徵圖
- 每個捕捉複雜的時頻模式
- 尺寸壓縮：128×282 → 16×35（降維90%）

### 6.5 Dropout（防止過擬合）

```python
Dropout(p=0.3)
```
**訓練時**：隨機丟棄30%的神經元
**測試時**：使用所有神經元
**作用**：強制網絡學習魯棒特徵，不依賴特定神經元

---

## 7. LSTM層：時序特徵建模

### 7.1 為什麼需要LSTM？

CNN提取的是**靜態空間特徵**，但情緒表達是**動態時序過程**：
- 語調的上升/下降
- 說話速度的變化
- 情緒的積累和爆發

**LSTM的優勢**：
- 記憶長期依賴（記住3秒前的聲調）
- 建模時序關係（捕捉語調變化趨勢）
- 處理變長序列

### 7.2 重塑數據為序列

```python
# CNN輸出：(batch, 128, 16, 35)
x = x.permute(0, 3, 1, 2)  # 調整維度順序
x = x.reshape(batch, 35, 128*16)  # 合併頻率和通道維度

# 結果：(batch, 35, 2048)
#         ↓     ↓    ↓
#       批次  時間步  特徵維度
```

**解讀**：
- **35個時間步**：對應35個時間幀（經過3次池化後）
- **每個時間步有2048個特徵**：來自128通道×16頻率位置
- **序列化**：將空間特徵轉為時間序列，供LSTM處理

### 7.3 LSTM單元工作原理

#### 標準LSTM結構
```
        ┌───────────────┐
ht-1 ──→│               │──→ ht (隱藏狀態)
        │  LSTM Cell    │
xt ────→│               │──→ ct (細胞狀態)
        └───────────────┘
```

**三個門控機制**：

1. **遺忘門 (Forget Gate)**
```python
ft = σ(Wf·[ht-1, xt] + bf)
作用：決定丟棄多少舊記憶
例如：前面的語調是否影響當前情緒判斷
```

2. **輸入門 (Input Gate)**
```python
it = σ(Wi·[ht-1, xt] + bi)
作用：決定添加多少新信息
例如：當前時間幀的特徵有多重要
```

3. **輸出門 (Output Gate)**
```python
ot = σ(Wo·[ht-1, xt] + bo)
作用：決定輸出多少細胞狀態
例如：當前時刻要傳遞多少情緒信息
```

**細胞狀態更新**：
```python
ct = ft ⊙ ct-1 + it ⊙ tanh(Wc·[ht-1, xt] + bc)
ht = ot ⊙ tanh(ct)
```

### 7.4 雙向LSTM (Bidirectional)

```python
LSTM(input_size=2048, hidden_size=128, num_layers=2, bidirectional=True)
```

**雙向處理**：
```
前向LSTM： [t1] → [t2] → [t3] → ... → [t35]  (看過去)
後向LSTM： [t35] ← [t34] ← [t33] ← ... ← [t1]  (看未來)

組合輸出： ht = [h_forward; h_backward]  (拼接)
```

**優勢**：
- 同時考慮前後文信息
- 例如：判斷當前聲調是「上升趨勢」還是「短暫波動」

**輸出形狀**：
```
(batch, 35, 256)
         ↓   ↓
      時間步  128(前向) + 128(後向)
```

### 7.5 多層堆疊（2層）

```
Layer 1:  輸入(2048) → LSTM → 輸出(256)
                              ↓
Layer 2:            輸入(256) → LSTM → 輸出(256)
```

**作用**：
- 第1層：學習基礎時序模式（音調變化）
- 第2層：學習高級時序模式（情緒演變邏輯）

### 7.6 選取最後時間步

```python
lstm_out, (h_n, c_n) = self.lstm(x)
# lstm_out: (batch, 35, 256)

x = lstm_out[:, -1, :]  # 取最後一個時間步
# x: (batch, 256)
```

**原因**：
- 最後時間步包含了**整個序列的匯總信息**
- 相當於「看完整段語音後的綜合判斷」

---

## 8. 全連接層：情緒分類

### 8.1 分類頭結構

```python
Dropout(0.3)
   ↓
FC1: Linear(256 → 128) + ReLU
   ↓
Dropout(0.3)
   ↓
FC2: Linear(128 → 5)
   ↓
輸出：5個邏輯值（未歸一化的類別分數）
```

### 8.2 第一個全連接層

```python
self.fc1 = nn.Linear(256, 128)
x = F.relu(self.fc1(x))
```

**作用**：
- 降維（256→128）
- 學習情緒特徵的非線性組合
- ReLU增強表達能力

### 8.3 第二個全連接層（輸出層）

```python
self.fc2 = nn.Linear(128, 5)
output = self.fc2(x)  # (batch, 5)
```

**輸出解釋**：
```
樣本1的輸出：[2.3, -1.1, 0.8, 3.5, -0.2]
              ↓     ↓    ↓    ↓     ↓
           類別0  類別1 類別2 類別3 類別4

最大值 = 3.5 (類別3) → 預測為 Emotion 4
```

### 8.4 損失函數：交叉熵

```python
criterion = nn.CrossEntropyLoss()
loss = criterion(outputs, labels)
```

**內部步驟**：

1. **Softmax歸一化**：
```python
softmax([2.3, -1.1, 0.8, 3.5, -0.2])
= [0.09, 0.03, 0.20, 0.66, 0.07]  # 轉為概率
```

2. **交叉熵計算**：
```python
如果真實標籤 = 類別3
Loss = -log(0.66) = 0.42

如果預測錯誤（真實標籤 = 類別0）
Loss = -log(0.09) = 2.41  (懲罰更大)
```

**優化目標**：讓正確類別的概率接近1

---

## 9. 數據增強技術

### 9.1 為什麼需要數據增強？

| 問題 | 解決方案 |
|------|----------|
| 訓練數據不足（630樣本） | 增強至3,150樣本（5倍） |
| 過擬合風險 | 引入隨機變化，提高泛化能力 |
| 實際環境差異 | 模擬噪聲、音量變化等現實情況 |

### 9.2 三種增強方法

#### 方法1：添加高斯噪聲
```python
def add_noise_torch(data, noise_factor=0.008):
    noise = torch.randn_like(data) * noise_factor
    return data + noise
```

**效果**：
- 模擬錄音環境噪聲
- 提高對噪聲的魯棒性
- 原始值 + 隨機擾動（標準差0.008）

**示例**：
```
原始值：-35.2 dB
增強後：-35.2 + 隨機(-0.02~0.02) = -35.18 dB
```

#### 方法2：時間位移
```python
def time_shift_torch(data, shift_max=0.25):
    shift = int(data.shape[-1] * 0.25 * random())
    return torch.roll(data, shifts=shift, dims=-1)
```

**效果**：
- 模擬說話時間差異
- 提高對時間對齊的魯棒性
- 最多位移25%時間幀（約70幀）

**示例**：
```
原始： [A, B, C, D, E]
位移+2：[D, E, A, B, C]  (循環右移)
```

#### 方法3：音量縮放
```python
def volume_scale_torch(data, scale_range=(0.7, 1.3)):
    scale = random(0.7, 1.3)
    return data * scale
```

**效果**：
- 模擬不同距離/音量
- 提高對響度變化的魯棒性
- 隨機縮放70%~130%

**示例**：
```
原始能量：-40 dB
縮放0.9倍：-40 + 20*log10(0.9) ≈ -40.9 dB
```

### 9.3 增強策略

```python
# 每個批次增強5倍
for _ in range(4):  # 原始數據已有1份
    應用概率：
    - 70% 添加噪聲
    - 70% 時間位移
    - 70% 音量縮放
```

**組合效果**：
- 1個樣本 → 5個變體
- 630訓練樣本 → 3,150個訓練實例
- 每個epoch看到不同的增強版本（隨機性）

---

## 10. 訓練策略

### 10.1 優化器配置

```python
optimizer = Adam(lr=0.001, weight_decay=1e-5)
```

**參數**：
- **學習率**：0.001（適中的初始步長）
- **權重衰減**：1e-5（L2正則化，防止過擬合）

**Adam優化器優勢**：
- 自適應學習率（每個參數獨立調整）
- 動量機制（加速收斂）
- 對噪聲不敏感

### 10.2 學習率調度

```python
scheduler = ReduceLROnPlateau(
    mode='min',
    factor=0.5,
    patience=5
)
```

**策略**：
- 監控驗證損失
- 5個epoch沒改善 → 學習率減半
- 防止困在局部最優

**示例**：
```
Epoch 1-10: lr = 0.001
Epoch 11-20: lr = 0.0005  (減半)
Epoch 21-30: lr = 0.00025 (再減半)
```

### 10.3 早停機制

```python
patience = 35
if no_improvement_for_35_epochs:
    stop_training()
```

**作用**：
- 驗證準確率不再提升時停止
- 防止過擬合
- 節省計算資源

**實際結果**：
- 最多訓練200 epochs
- 通常在60-80 epoch收斂

### 10.4 訓練循環偽代碼

```python
for epoch in range(200):
    # 訓練階段
    model.train()
    for batch_x, batch_y in train_loader:
        # 1. 數據增強（5倍）
        batch_x_aug, batch_y_aug = augment(batch_x, batch_y)

        # 2. 前向傳播
        outputs = model(batch_x_aug)
        loss = criterion(outputs, batch_y_aug)

        # 3. 反向傳播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # 驗證階段
    model.eval()
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            outputs = model(batch_x)  # 不增強
            val_loss = criterion(outputs, batch_y)

    # 4. 學習率調整
    scheduler.step(val_loss)

    # 5. 保存最佳模型
    if val_acc > best_val_acc:
        save_model()

    # 6. 早停檢查
    if patience_exceeded:
        break
```

---

## 11. 模型性能分析

### 11.1 整體結果

| 指標 | 數值 |
|------|------|
| **測試準確率** | **87.41%** |
| **訓練時間** | ~30-40分鐘（RTX 3060） |
| **參數量** | 2,752,197 |
| **最佳驗證準確率** | ~88-90% |

### 11.2 各類別性能

| 情緒類別 | 精確率 | 召回率 | F1分數 | 支持樣本數 |
|---------|--------|--------|--------|-----------|
| Emotion 1 | 86.21% | 89.29% | 87.72% | 28 |
| Emotion 2 | 92.00% | 85.19% | 88.46% | 27 |
| Emotion 3 | 95.24% | 74.07% | 83.33% | 27 |
| Emotion 4 | 83.33% | **96.15%** | 89.29% | 26 |
| Emotion 5 | 83.33% | 92.59% | 87.72% | 27 |

**宏平均**：88.02% 精確率，87.46% 召回率

### 11.3 混淆矩陣分析（示例）

```
預測 →    E1   E2   E3   E4   E5
真實 ↓
E1         25   1    0    2    0    (89% 正確)
E2         2    23   1    1    0    (85% 正確)
E3         0    3    20   2    2    (74% 正確)
E4         0    0    1    25   0    (96% 正確)
E5         1    0    1    0    25   (93% 正確)
```

**觀察**：
- **Emotion 4** 識別最準確（96.15%）
- **Emotion 3** 相對較難（74.07%），易與E2/E4混淆
- 整體混淆較少，說明特徵區分度好

### 11.4 訓練曲線特徵

**典型訓練過程**：
```
Epoch 1-10:   快速下降，準確率60% → 75%
Epoch 11-30:  穩定提升，75% → 85%
Epoch 31-60:  微調階段，85% → 88%
Epoch 61+:    趨於穩定，驗證集不再提升 → 早停
```

**過擬合控制效果**：
- 訓練準確率：~92%
- 驗證準確率：~88%
- 差距僅4%（良好的泛化能力）

---

## 12. 完整數據流示例

讓我們跟蹤一個實際樣本的完整處理過程：

### 樣本：一段3秒的「憤怒」情緒語音

#### Step 1: 輸入原始音頻
```
形狀：(144000,)
採樣率：48kHz
時長：3秒
數值範圍：[-1.0, 1.0]（歸一化波形）
```

#### Step 2: Mel頻譜圖轉換
```
輸入：(144000,)
處理：STFT + Mel濾波 + log轉換
輸出：(128, 282)
  ↓
頻譜特徵：
- 低頻（0-1000Hz）：基頻能量
- 中頻（1000-4000Hz）：formants（共振峰）
- 高頻（4000-8000Hz）：噪聲成分
```

**憤怒情緒特徵**：
- 高頻能量強（聲帶緊張）
- 頻率變化劇烈（語調起伏大）
- 能量集中在特定時間段（情緒爆發點）

#### Step 3: 增加通道維度
```
(128, 282) → (1, 128, 282)
```
類似於灰度圖像（單通道）

#### Step 4: CNN特徵提取

**Conv1處理**：
```
輸入：(1, 128, 282)
操作：32個3×3濾波器掃描
學到：
  - 濾波器7：檢測到「高頻能量峰」
  - 濾波器15：檢測到「頻率快速上升」
輸出：(32, 64, 141)
```

**Conv2處理**：
```
輸入：(32, 64, 141)
操作：64個濾波器組合前層特徵
學到：
  - 濾波器23：「高頻峰 + 頻率上升」→ 憤怒特徵
輸出：(64, 32, 70)
```

**Conv3處理**：
```
輸入：(64, 32, 70)
操作：128個濾波器提取高級模式
學到：
  - 濾波器89：「憤怒聲調模式」（綜合特徵）
輸出：(128, 16, 35)
```

#### Step 5: 重塑為時間序列
```
(128, 16, 35) → (35, 2048)
  ↓             ↓     ↓
空間特徵      時間步  特徵
```

**解讀**：
- 35個時間步對應35個時間段
- 每個時間步有2048維特徵向量
- 序列：[t1, t2, ..., t35]

#### Step 6: LSTM時序建模

**時間步1（開始）**：
```
輸入：t1 特徵 (2048維)
LSTM狀態：初始化
輸出：h1 (256維)
語義：「檢測到正常語調」
```

**時間步15（中間）**：
```
輸入：t15 特徵
記憶：前14個時間步的累積信息
輸出：h15
語義：「語調開始上升，可能是情緒變化」
```

**時間步35（結束）**：
```
輸入：t35 特徵
記憶：整段語音的完整信息
輸出：h35 (256維)
語義：「整體判斷：高頻能量 + 劇烈變化 + 持續升調 → 憤怒」
```

#### Step 7: 全連接分類
```
輸入：h35 (256維)
  ↓
FC1 + ReLU → (128維)
  ↓
FC2 → (5維原始分數)

輸出：[-1.2, 0.3, -0.8, 4.5, -2.1]
       ↓     ↓    ↓    ↓     ↓
      E1    E2   E3   E4    E5

Softmax後：[0.02, 0.08, 0.03, 0.85, 0.01]

預測：Emotion 4（85%置信度）
```

#### Step 8: 損失計算與反向傳播

**如果真實標籤 = Emotion 4**：
```
Loss = -log(0.85) = 0.16（低損失，預測準確）

反向傳播：
- 微調FC層權重，讓憤怒特徵更明確
- 調整LSTM，強化時序模式識別
- 優化CNN濾波器，更好捕捉高頻特徵
```

---

## 總結：CNN+LSTM如何識別情緒

### 核心工作流程

1. **特徵工程（Mel頻譜圖）**：
   - 將3秒音頻轉為128×282的時頻圖
   - 保留人耳感知的關鍵頻率信息

2. **空間特徵提取（CNN）**：
   - 3層卷積逐步提取從簡單到複雜的模式
   - 學習「什麼樣的聲音特徵代表某種情緒」

3. **時序建模（LSTM）**：
   - 雙向LSTM捕捉前後文依賴
   - 學習「情緒如何隨時間演變」

4. **分類決策（全連接）**：
   - 綜合時空特徵做最終判斷
   - 輸出5類情緒的概率分佈

### 為什麼這個架構有效？

| 組件 | 處理的問題 | 類比 |
|------|-----------|------|
| **Mel頻譜圖** | 音頻數據高維且非結構化 | 將聲音「可視化」為圖像 |
| **CNN** | 提取局部特徵 | 像識別圖像中的「邊緣」「紋理」 |
| **LSTM** | 捕捉時序依賴 | 像理解故事情節的發展 |
| **組合效果** | 同時理解「內容」和「表達」 | 既看「說什麼」，又聽「怎麼說」 |

### 關鍵成功因素

1. **數據增強**：5倍擴增有效緩解小樣本問題
2. **正則化**：Dropout + 權重衰減防止過擬合
3. **架構設計**：CNN-LSTM互補，發揮各自優勢
4. **訓練策略**：學習率調度 + 早停確保最優性能

### 實際應用潛力

- **人機交互**：情感化對話系統
- **客服質量監控**：分析通話情緒
- **心理健康**：抑鬱/焦慮情緒檢測
- **娛樂產業**：遊戲/影視情感分析

---

## 附錄：代碼關鍵段落解析

### A. 模型定義核心
```python
class CNN_LSTM_EmotionClassifier(nn.Module):
    def forward(self, x):
        # 1. CNN提取空間特徵
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        # ... (重複2次)

        # 2. 重塑為時間序列
        x = x.permute(0, 3, 1, 2)
        x = x.reshape(batch_size, width, channels * height)

        # 3. LSTM建模時序
        lstm_out, _ = self.lstm(x)
        x = lstm_out[:, -1, :]  # 取最後時間步

        # 4. 全連接分類
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

### B. 數據增強應用
```python
def augment_batch(batch_x, batch_y, augmentation_factor=5):
    augmented = [batch_x]  # 原始數據
    for _ in range(4):  # 生成4個增強版本
        x_aug = batch_x.clone()
        if random() > 0.3:
            x_aug = add_noise(x_aug)
        if random() > 0.3:
            x_aug = time_shift(x_aug)
        if random() > 0.3:
            x_aug = volume_scale(x_aug)
        augmented.append(x_aug)
    return torch.cat(augmented)  # 5倍數據
```

### C. 訓練關鍵步驟
```python
# 訓練階段
batch_x_aug, batch_y_aug = augment_batch(batch_x, batch_y)
outputs = model(batch_x_aug)
loss = criterion(outputs, batch_y_aug)
loss.backward()
optimizer.step()

# 驗證階段（無增強）
with torch.no_grad():
    outputs = model(batch_x)
    val_loss = criterion(outputs, batch_y)
```

---

**文檔版本**：v1.0
**模型文件**：PyTorch_CNN_LSTM_EmotionRecognition.ipynb
**最佳模型**：best_cnn_lstm_model_v3.pth
**測試準確率**：87.41%
