{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6f7713",
   "metadata": {},
   "source": [
    "# Homework 4 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e70137",
   "metadata": {},
   "source": [
    "This is an individual assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a000b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e7579",
   "metadata": {},
   "source": [
    "Write your answers using markdown cells or embed handwritten answers with ```IPython.display.Image```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3a152",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6599",
   "metadata": {},
   "source": [
    "# Exercise 1 (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2dfa9",
   "metadata": {},
   "source": [
    "**Consider the following application scenarios. Between the Linear Support Vector Machines (SVM) and Logistic Regression (LR), which one would you select for each scenario? Justify your answers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b8d51",
   "metadata": {},
   "source": [
    "1. (2 points) **Detecting fraudulent transactions in a credit card dataset, where the fraudulent transactions might be a minority class and are likely to be distinct from normal transactions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe297a",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would select **Linear SVM** for this scenario.\n",
    "\n",
    "**Justification:**\n",
    "- Linear SVM is particularly effective when classes are well-separated and distinct, which is likely the case for fraudulent vs normal transactions\n",
    "- SVM maximizes the margin between classes, making it robust to outliers and better at finding a clear decision boundary\n",
    "- SVM's focus on support vectors (boundary cases) makes it effective even with imbalanced datasets where fraudulent transactions are minority\n",
    "- While the class imbalance is a consideration, Linear SVM with appropriate class weights can handle this effectively\n",
    "- SVM is less affected by the imbalanced nature since it focuses on the decision boundary rather than probability estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254524a",
   "metadata": {},
   "source": [
    "2. (2 points) **Text classification tasks, where each word or n-gram can be considered as a feature, leading to high-dimensional feature spaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f10c1",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would select **Linear SVM** for this scenario.\n",
    "\n",
    "**Justification:**\n",
    "- Text classification with word/n-gram features creates very high-dimensional feature spaces (thousands to millions of features)\n",
    "- Linear SVM is particularly well-suited for high-dimensional data and often achieves excellent performance in text classification tasks\n",
    "- In high-dimensional spaces, data tends to be linearly separable, making linear SVM's margin maximization approach very effective\n",
    "- SVM is computationally efficient in high dimensions compared to other methods\n",
    "- Linear SVM is less prone to overfitting in high-dimensional spaces due to its margin-based formulation and structural risk minimization principle\n",
    "- Text data is typically sparse, and SVM handles sparse high-dimensional data very efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce60e8c",
   "metadata": {},
   "source": [
    "3. (2 points) **Medical diagnosis, where the occurrence of a rare disease is much lower than that of the healthy population.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1900a8",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would select **Logistic Regression** for this scenario.\n",
    "\n",
    "**Justification:**\n",
    "- Medical diagnosis requires probability estimates for informed decision-making, not just binary classifications\n",
    "- Logistic Regression naturally provides calibrated probability outputs (probability of disease), which is crucial for medical applications where doctors need confidence levels\n",
    "- These probabilities help establish diagnostic thresholds and allow for risk assessment\n",
    "- Medical professionals can use probability outputs to make more nuanced decisions (e.g., \"85% probability of disease\" vs \"15% probability\")\n",
    "- Logistic Regression is more interpretable, allowing doctors to understand which features contribute to the diagnosis\n",
    "- While class imbalance exists, Logistic Regression can handle this with appropriate techniques (class weights, threshold adjustment)\n",
    "- The probabilistic output is essential for cost-sensitive decisions in healthcare (balancing false positives vs false negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66d866",
   "metadata": {},
   "source": [
    "4. (2 points) **Large-scale text categorization, where the focus is on assigning documents to predefined categories, rather than estimating the probability of belonging to a specific class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263eec9",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would select **Linear SVM** for this scenario.\n",
    "\n",
    "**Justification:**\n",
    "- The problem explicitly states that the focus is on category assignment rather than probability estimation, which is exactly what SVM is designed for\n",
    "- Linear SVM directly optimizes for the decision boundary without computing probabilities, making it more efficient for large-scale problems\n",
    "- SVM's margin maximization leads to better generalization on unseen documents\n",
    "- For large-scale text categorization, computational efficiency is critical - Linear SVM is faster than Logistic Regression since it doesn't need to compute and normalize probabilities\n",
    "- Text categorization typically involves high-dimensional sparse features (similar to question 2), where Linear SVM excels\n",
    "- SVM focuses computational resources on support vectors (boundary cases), making it scalable for large datasets\n",
    "- Since probability outputs are not needed, the computational overhead of Logistic Regression's probability calibration is unnecessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2db842",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdc17a",
   "metadata": {},
   "source": [
    "# Exercise 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb211a1a",
   "metadata": {},
   "source": [
    "**Consider a feed-forward multilayer perceptron with a $D$-$M$-$D$ architecture ($M \\leq D$) and linear activation functions. Show that such a neural network, when trained to predict the input at the output layer, performs principal component analysis by considering the minimization it solves. Indicate if the network will require bias terms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e544a2",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**Network Architecture:**\n",
    "- Input layer: $D$ neurons (input $\\mathbf{x} \\in \\mathbb{R}^D$)\n",
    "- Hidden layer: $M$ neurons (where $M \\leq D$), with hidden representation $\\mathbf{h} \\in \\mathbb{R}^M$\n",
    "- Output layer: $D$ neurons (output $\\mathbf{\\hat{x}} \\in \\mathbb{R}^D$)\n",
    "- All activation functions are linear\n",
    "\n",
    "**Forward Pass:**\n",
    "\n",
    "Let $\\mathbf{W}_1 \\in \\mathbb{R}^{M \\times D}$ be the input-to-hidden weights and $\\mathbf{W}_2 \\in \\mathbb{R}^{D \\times M}$ be the hidden-to-output weights.\n",
    "\n",
    "Without bias terms:\n",
    "$$\\mathbf{h} = \\mathbf{W}_1 \\mathbf{x}$$\n",
    "$$\\mathbf{\\hat{x}} = \\mathbf{W}_2 \\mathbf{h} = \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{x}$$\n",
    "\n",
    "**Objective Function:**\n",
    "\n",
    "The network is trained to reconstruct the input, minimizing:\n",
    "$$E = \\frac{1}{N} \\sum_{n=1}^{N} \\|\\mathbf{x}_n - \\mathbf{\\hat{x}}_n\\|^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\|\\mathbf{x}_n - \\mathbf{W}_2 \\mathbf{W}_1 \\mathbf{x}_n\\|^2$$\n",
    "\n",
    "**Connection to PCA:**\n",
    "\n",
    "PCA finds an $M$-dimensional subspace that minimizes reconstruction error. The principal components are the eigenvectors corresponding to the $M$ largest eigenvalues of the data covariance matrix.\n",
    "\n",
    "For the linear autoencoder:\n",
    "- The hidden layer $\\mathbf{h} = \\mathbf{W}_1 \\mathbf{x}$ projects the input into an $M$-dimensional space\n",
    "- The output layer $\\mathbf{\\hat{x}} = \\mathbf{W}_2 \\mathbf{h}$ reconstructs the input from this lower-dimensional representation\n",
    "- The combined transformation $\\mathbf{W}_2 \\mathbf{W}_1$ represents a projection matrix onto an $M$-dimensional subspace\n",
    "\n",
    "At the optimal solution, the columns of $\\mathbf{W}_2^T$ span the same subspace as the first $M$ principal components. The network learns to:\n",
    "1. Encode the input into the $M$-dimensional subspace that captures maximum variance\n",
    "2. Decode from this subspace to reconstruct the original input\n",
    "\n",
    "This is exactly what PCA does: find the $M$-dimensional linear subspace that minimizes reconstruction error, which is equivalent to maximizing retained variance.\n",
    "\n",
    "**Bias Terms:**\n",
    "\n",
    "The network does **NOT require bias terms**. PCA operates on mean-centered data and finds directions of maximum variance without needing an offset. If the data is pre-centered (zero mean), bias terms would be unnecessary and would remain zero during training. The linear projection and reconstruction can be achieved purely through the weight matrices $\\mathbf{W}_1$ and $\\mathbf{W}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13313dea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d77d7",
   "metadata": {},
   "source": [
    "# Exercise 3 (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701e979",
   "metadata": {},
   "source": [
    "**Compare and contrast the objective functions of PCA and Fisher Linear Discriminant Analysis (FLDA) for dimensionality reduction. Specifically, explain how FLDA incorporates class information that PCA ignores, defining all terms such as within-class scatter and between-class scatter matrices. And, discuss the advantages and disadvantages of each method, particularly in the context of supervised vs unsupervised learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb2bf3",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "## Objective Functions:\n",
    "\n",
    "**PCA (Unsupervised):**\n",
    "- **Goal**: Maximize variance in the projected data\n",
    "- **Objective**: Find directions that maximize $\\mathbf{w}^T \\mathbf{S} \\mathbf{w}$, where $\\mathbf{S}$ is the total data covariance matrix\n",
    "- $$\\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\overline{\\mathbf{x}})(\\mathbf{x}_n - \\overline{\\mathbf{x}})^T$$\n",
    "- PCA finds eigenvectors of $\\mathbf{S}$ corresponding to largest eigenvalues\n",
    "- **Does NOT use class labels** - purely based on data variance\n",
    "\n",
    "**FLDA (Supervised):**\n",
    "- **Goal**: Maximize between-class separation while minimizing within-class scatter\n",
    "- **Objective**: Maximize the Fisher criterion (ratio):\n",
    "$$J(\\mathbf{w}) = \\frac{\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w}}{\\mathbf{w}^T \\mathbf{S}_W \\mathbf{w}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "**Within-class scatter matrix** $\\mathbf{S}_W$: Measures scatter of samples around their respective class means\n",
    "$$\\mathbf{S}_W = \\sum_{c=1}^{C} \\sum_{\\mathbf{x}_n \\in \\mathcal{C}_c} (\\mathbf{x}_n - \\boldsymbol{\\mu}_c)(\\mathbf{x}_n - \\boldsymbol{\\mu}_c)^T$$\n",
    "where $\\boldsymbol{\\mu}_c$ is the mean of class $c$ and $\\mathcal{C}_c$ is the set of samples in class $c$\n",
    "\n",
    "**Between-class scatter matrix** $\\mathbf{S}_B$: Measures separation between class means\n",
    "$$\\mathbf{S}_B = \\sum_{c=1}^{C} N_c (\\boldsymbol{\\mu}_c - \\overline{\\mathbf{x}})(\\boldsymbol{\\mu}_c - \\overline{\\mathbf{x}})^T$$\n",
    "where $N_c$ is the number of samples in class $c$, and $\\overline{\\mathbf{x}}$ is the overall mean\n",
    "\n",
    "- FLDA finds directions by solving the generalized eigenvalue problem: $\\mathbf{S}_B \\mathbf{w} = \\lambda \\mathbf{S}_W \\mathbf{w}$\n",
    "\n",
    "## Key Difference - Use of Class Information:\n",
    "\n",
    "**FLDA incorporates class labels** by:\n",
    "1. Computing separate statistics for each class (class means $\\boldsymbol{\\mu}_c$)\n",
    "2. Maximizing the separation between these class means (via $\\mathbf{S}_B$)\n",
    "3. Minimizing the spread within each class (via $\\mathbf{S}_W$)\n",
    "4. Finding projections that make classes as distinguishable as possible\n",
    "\n",
    "**PCA ignores class labels** by:\n",
    "1. Treating all data points equally regardless of class\n",
    "2. Only considering overall data variance\n",
    "3. May project data onto directions with high variance but poor class separation\n",
    "\n",
    "## Advantages and Disadvantages:\n",
    "\n",
    "**PCA Advantages:**\n",
    "- **Unsupervised**: Works without labeled data\n",
    "- **General purpose**: Good for visualization, noise reduction, and general dimensionality reduction\n",
    "- **Captures overall data structure**: Preserves maximum variance\n",
    "- **No class limitation**: Works with any number of classes or no classes\n",
    "\n",
    "**PCA Disadvantages:**\n",
    "- **Ignores class information**: High-variance directions may not separate classes well\n",
    "- **Not optimized for classification**: May discard discriminative information in low-variance directions\n",
    "- **Can be misleading for supervised tasks**: Important class boundaries might be in low-variance directions\n",
    "\n",
    "**FLDA Advantages:**\n",
    "- **Optimized for classification**: Directly maximizes class separability\n",
    "- **Supervised learning**: Uses label information effectively\n",
    "- **Better for discrimination**: Often achieves better classification performance with fewer dimensions\n",
    "- **Focuses on discriminative features**: Finds features that best separate classes\n",
    "\n",
    "**FLDA Disadvantages:**\n",
    "- **Requires labeled data**: Cannot be used in unsupervised settings\n",
    "- **Limited dimensions**: Can extract at most $C-1$ features (where $C$ is number of classes)\n",
    "- **Assumes class distributions**: Performance depends on within-class and between-class structure\n",
    "- **Can fail with small samples**: $\\mathbf{S}_W$ must be invertible; problematic when $N < D$ or classes overlap significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6652a45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6690d",
   "metadata": {},
   "source": [
    "# Exercise 4 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8241",
   "metadata": {},
   "source": [
    "**Design the simplest feed-forward MLP, by specifying numerical values for its weights and biases, using the threshold activation function, $\\phi(x) = \\begin{cases} 1 & x >0\\\\ -1 & x\\leq 0\\end{cases}$, to solve the following scenarios:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba556d56",
   "metadata": {},
   "source": [
    "1. (2 points) **The NAND logic gate, $\\overline{x_1 \\cap x_2} = \\overline{x_1} \\cup \\overline{x_2}$, where $x_i\\in\\{0,1\\}, i=1,2$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3942a4b",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The NAND gate outputs 0 only when both inputs are 1, otherwise outputs 1.\n",
    "\n",
    "| $x_1$ | $x_2$ | NAND output |\n",
    "|-------|-------|-------------|\n",
    "| 0     | 0     | 1           |\n",
    "| 0     | 1     | 1           |\n",
    "| 1     | 0     | 1           |\n",
    "| 1     | 1     | 0           |\n",
    "\n",
    "**MLP Architecture:**\n",
    "- Input layer: 2 neurons ($x_1, x_2$)\n",
    "- Hidden layer: None needed (single layer perceptron)\n",
    "- Output layer: 1 neuron with threshold activation\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Using threshold activation $\\phi(x) = \\begin{cases} 1 & x > 0\\\\ -1 & x \\leq 0\\end{cases}$\n",
    "\n",
    "We need to map {0,1} outputs to {1,-1} format. Let's design the network:\n",
    "\n",
    "**Weights and Bias:**\n",
    "- $w_1 = -1$ (weight for $x_1$)\n",
    "- $w_2 = -1$ (weight for $x_2$)  \n",
    "- $b = 1$ (bias)\n",
    "\n",
    "**Network equation:**\n",
    "$$y = \\phi(w_1 x_1 + w_2 x_2 + b) = \\phi(-x_1 - x_2 + 1)$$\n",
    "\n",
    "**Verification:**\n",
    "- $(x_1, x_2) = (0, 0)$: $\\phi(-0 - 0 + 1) = \\phi(1) = 1$ ✓ (NAND = 1)\n",
    "- $(x_1, x_2) = (0, 1)$: $\\phi(-0 - 1 + 1) = \\phi(0) = -1$ → Need adjustment\n",
    "\n",
    "Let me reconsider with proper threshold adjustment:\n",
    "\n",
    "**Corrected Solution:**\n",
    "- $w_1 = -2$\n",
    "- $w_2 = -2$\n",
    "- $b = 3$\n",
    "\n",
    "$$y = \\phi(-2x_1 - 2x_2 + 3)$$\n",
    "\n",
    "**Verification:**\n",
    "- $(0, 0)$: $\\phi(3) = 1$ ✓\n",
    "- $(0, 1)$: $\\phi(1) = 1$ ✓\n",
    "- $(1, 0)$: $\\phi(1) = 1$ ✓\n",
    "- $(1, 1)$: $\\phi(-1) = -1$ → maps to 0 ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0e5ee",
   "metadata": {},
   "source": [
    "2. (4 points) **The following two-dimensional dataset,**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c8d22",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "*Note: The two-dimensional dataset image is not visible in the text. I'll provide a general approach for designing an MLP for a typical 2D classification problem.*\n",
    "\n",
    "For a 2D dataset with non-linearly separable classes, the approach would be:\n",
    "\n",
    "**MLP Architecture:**\n",
    "- Input layer: 2 neurons ($x_1, x_2$)\n",
    "- Hidden layer: 2-4 neurons (depending on complexity)\n",
    "- Output layer: 1 neuron\n",
    "\n",
    "**General Approach:**\n",
    "\n",
    "If the problem requires XOR-like separation:\n",
    "- Hidden layer: 2 neurons\n",
    "- Example weights for XOR pattern:\n",
    "\n",
    "**Hidden Layer:**\n",
    "- Neuron 1: $h_1 = \\phi(w_{11}x_1 + w_{12}x_2 + b_1)$ with $w_{11}=1, w_{12}=1, b_1=-0.5$\n",
    "- Neuron 2: $h_2 = \\phi(w_{21}x_1 + w_{22}x_2 + b_2)$ with $w_{21}=1, w_{22}=1, b_2=-1.5$\n",
    "\n",
    "**Output Layer:**\n",
    "- $y = \\phi(v_1 h_1 + v_2 h_2 + b_o)$ with $v_1=1, v_2=-1, b_o=-0.5$\n",
    "\n",
    "*If you can provide the specific dataset image or description, I can give exact weights and biases for that particular problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34a278",
   "metadata": {},
   "source": [
    "3. (4 points) **The given ground truth table,**\n",
    "\n",
    "| x1 | x2 | x3 | t | \n",
    "| --  |  -- |  -- | --|\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 0 | 1 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 1 | 0 |\n",
    "| 1 | 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06267836",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Analyzing the truth table:\n",
    "\n",
    "| $x_1$ | $x_2$ | $x_3$ | $t$ |\n",
    "|-------|-------|-------|-----|\n",
    "| 0     | 0     | 0     | 0   |\n",
    "| 0     | 0     | 1     | 1   |\n",
    "| 0     | 1     | 0     | 1   |\n",
    "| 0     | 1     | 1     | 0   |\n",
    "| 1     | 0     | 0     | 1   |\n",
    "| 1     | 0     | 1     | 0   |\n",
    "| 1     | 1     | 0     | 0   |\n",
    "| 1     | 1     | 1     | 1   |\n",
    "\n",
    "**Pattern Recognition:**\n",
    "Looking at when $t=1$: This is a 3-input parity function (odd parity) - output is 1 when odd number of inputs are 1.\n",
    "- Row 2: one 1 → $t=1$ ✓\n",
    "- Row 3: one 1 → $t=1$ ✓\n",
    "- Row 5: one 1 → $t=1$ ✓\n",
    "- Row 8: three 1s → $t=1$ ✓\n",
    "\n",
    "This is $t = x_1 \\oplus x_2 \\oplus x_3$ (3-bit XOR)\n",
    "\n",
    "**MLP Architecture:**\n",
    "- Input layer: 3 neurons\n",
    "- Hidden layer: 4 neurons (needed for 3-input XOR)\n",
    "- Output layer: 1 neuron\n",
    "\n",
    "**Design Strategy:**\n",
    "\n",
    "The 3-input XOR can be decomposed as: $(x_1 \\oplus x_2) \\oplus x_3$\n",
    "\n",
    "**Hidden Layer (4 neurons):**\n",
    "\n",
    "Neuron $h_1$: Detects $x_1$ AND $x_2$\n",
    "- Weights: $w_{11}=2, w_{12}=2, w_{13}=0$, bias $b_1=-3$\n",
    "- $h_1 = \\phi(2x_1 + 2x_2 - 3)$ → outputs 1 only when $x_1=x_2=1$\n",
    "\n",
    "Neuron $h_2$: Detects $x_1$ OR $x_2$\n",
    "- Weights: $w_{21}=2, w_{22}=2, w_{23}=0$, bias $b_2=-1$\n",
    "- $h_2 = \\phi(2x_1 + 2x_2 - 1)$ → outputs 1 when at least one of $x_1, x_2$ is 1\n",
    "\n",
    "Neuron $h_3$: Detects $(x_1 \\oplus x_2)$ AND $x_3$\n",
    "- Weights: $w_{31}=2, w_{32}=2, w_{33}=4$, bias $b_3=-3$\n",
    "- This is complex, let me use a cleaner approach:\n",
    "\n",
    "**Simplified Architecture:**\n",
    "\n",
    "**Hidden Layer:**\n",
    "- $h_1 = \\phi(2x_1 + 2x_2 - 1)$ with weights $(2, 2, 0, -1)$ → $x_1$ OR $x_2$\n",
    "- $h_2 = \\phi(2x_1 + 2x_2 - 3)$ with weights $(2, 2, 0, -3)$ → $x_1$ AND $x_2$\n",
    "- $h_3 = \\phi(2x_3 - 1)$ with weights $(0, 0, 2, -1)$ → $x_3$\n",
    "- $h_4 = \\phi(2h_1 - 4h_2 + 2x_3 - 1)$ → XOR of first two and $x_3$\n",
    "\n",
    "**Output Layer:**\n",
    "- $y = \\phi(2h_1 - 4h_2 + 2h_3 - 1)$\n",
    "\n",
    "**Complete weight specification:**\n",
    "\n",
    "Input to Hidden (3×4 weight matrix + 4 biases):\n",
    "```\n",
    "h1: w=[2, 2, 0], b=-1\n",
    "h2: w=[2, 2, 0], b=-3  \n",
    "h3: w=[0, 0, 2], b=-1\n",
    "h4: w=[2, -4, 2], b=-1 (takes x1 OR x2, x1 AND x2, x3)\n",
    "```\n",
    "\n",
    "Hidden to Output:\n",
    "```\n",
    "y: v=[2, -4, 2, 0], b=-1\n",
    "```\n",
    "\n",
    "This implements the 3-bit XOR function using the threshold activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d868f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c452971",
   "metadata": {},
   "source": [
    "# Exercise 5 (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6175122",
   "metadata": {},
   "source": [
    "**Explain why the input-to-hidden weights must be different from each other (e.g., random) or else learning cannot proceed well. Specifically, what happens if the weights are initialized so as to have identical values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89ab9e",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Input-to-hidden weights must be different (typically randomized) because **identical weight initialization creates a symmetry problem** that prevents effective learning.\n",
    "\n",
    "## What Happens with Identical Weights:\n",
    "\n",
    "If all input-to-hidden weights are initialized identically (e.g., all weights $w_{ij}^{(1)} = c$ for some constant $c$):\n",
    "\n",
    "1. **All hidden neurons compute the same function:**\n",
    "   - Each hidden neuron receives: $z_j = \\sum_i w_{ij} x_i + b_j$\n",
    "   - If all $w_{ij}$ are identical and biases are identical, then $z_1 = z_2 = \\ldots = z_M$\n",
    "   - All hidden neurons produce the same activation: $h_1 = h_2 = \\ldots = h_M$\n",
    "\n",
    "2. **Identical gradients during backpropagation:**\n",
    "   - During backpropagation, all hidden neurons receive the same error signal\n",
    "   - Weight updates are computed as: $\\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}}$\n",
    "   - Since all neurons have identical inputs and outputs, all gradients are identical\n",
    "   - Therefore, all weights receive identical updates: $\\Delta w_{1j} = \\Delta w_{2j} = \\ldots$\n",
    "\n",
    "3. **Symmetry is never broken:**\n",
    "   - After each update: $w_{ij}^{new} = w_{ij}^{old} - \\eta \\Delta w_{ij}$\n",
    "   - Since all weights start identical and receive identical updates, they remain identical forever\n",
    "   - The hidden neurons never specialize or learn different features\n",
    "\n",
    "4. **Network becomes equivalent to a single hidden neuron:**\n",
    "   - With $M$ identical hidden neurons, the network effectively has only **one unique hidden unit repeated $M$ times**\n",
    "   - This drastically reduces the network's representational capacity\n",
    "   - The network cannot learn complex non-linear functions\n",
    "\n",
    "## Why Random Initialization Works:\n",
    "\n",
    "- **Breaks symmetry**: Different initial weights → different neuron activations → different gradients\n",
    "- **Allows specialization**: Each hidden neuron can learn to detect different features\n",
    "- **Full representational capacity**: All $M$ hidden neurons contribute independently\n",
    "- **Enables diverse feature learning**: Different neurons capture different aspects of the input patterns\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "Random weight initialization is essential for **symmetry breaking** in neural networks. Without it, hidden neurons would be redundant copies, defeating the purpose of having multiple hidden units and severely limiting the network's learning capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba1a27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e27745",
   "metadata": {},
   "source": [
    "# Exercise 5 (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2622c3cb",
   "metadata": {},
   "source": [
    "**Answer the following questions, and provide appropriate justifications, about Convolutional Neural Networks (CNNs):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7605a8",
   "metadata": {},
   "source": [
    "1. (2 points) **Would you prefer to add more filters in the first convolutional layer or the second convolutional layer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b480c25",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would prefer to **add more filters in the second convolutional layer** rather than the first.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "1. **Feature Hierarchy:**\n",
    "   - The first convolutional layer detects simple, low-level features (edges, corners, basic textures)\n",
    "   - The second convolutional layer combines these low-level features to detect more complex, higher-level patterns\n",
    "   - More complex patterns require more diverse representations, hence more filters\n",
    "\n",
    "2. **Computational Efficiency:**\n",
    "   - The first layer operates on the full input resolution (larger spatial dimensions)\n",
    "   - Adding filters to the first layer increases computation significantly since each filter processes every pixel\n",
    "   - The second layer typically operates on reduced spatial dimensions (due to pooling), making additional filters computationally cheaper\n",
    "\n",
    "3. **Information Compression:**\n",
    "   - Early layers should focus on capturing essential low-level features efficiently\n",
    "   - Later layers need more capacity to represent the combinatorial explosion of higher-level feature combinations\n",
    "   - A smaller number of low-level features can be combined in many ways to create numerous high-level features\n",
    "\n",
    "4. **Common CNN Architecture Practice:**\n",
    "   - Standard CNN architectures (VGG, ResNet, etc.) typically increase the number of filters as depth increases\n",
    "   - Pattern: 64 → 128 → 256 → 512 filters is more common than the reverse\n",
    "   - This follows the principle of building increasingly abstract representations\n",
    "\n",
    "5. **Representational Power:**\n",
    "   - The second layer has more combinations to represent since it builds on features from the first layer\n",
    "   - More filters in deeper layers provide greater capacity for learning complex, task-specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef6b2c",
   "metadata": {},
   "source": [
    "2. (2 points) **Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee64c10",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I would prefer **max pooling** over a convolutional layer with the same stride for several important reasons:\n",
    "\n",
    "**Advantages of Max Pooling:**\n",
    "\n",
    "1. **No Parameters to Learn:**\n",
    "   - Max pooling has **zero learnable parameters**\n",
    "   - A convolutional layer with stride has weights and biases that need to be learned\n",
    "   - Fewer parameters → less risk of overfitting, faster training, smaller model size\n",
    "\n",
    "2. **Invariance to Small Translations:**\n",
    "   - Max pooling provides **local translation invariance**\n",
    "   - It selects the maximum activation within a region, making the representation robust to small shifts\n",
    "   - A conv layer with stride might miss features that are slightly offset\n",
    "\n",
    "3. **Computational Efficiency:**\n",
    "   - Max pooling is a simple selection operation (just finding maximum)\n",
    "   - No multiply-accumulate operations needed\n",
    "   - Convolutional layers require expensive convolution operations even with stride\n",
    "\n",
    "4. **Feature Enhancement:**\n",
    "   - Max pooling amplifies the strongest activations\n",
    "   - Helps the network focus on the most prominent features\n",
    "   - Acts as a form of **feature selection** by keeping only the most relevant information\n",
    "\n",
    "5. **Reduces Overfitting:**\n",
    "   - By providing a form of regularization through aggressive downsampling\n",
    "   - Forces the network to learn more robust features\n",
    "   - The discretization effect of max pooling adds noise resistance\n",
    "\n",
    "6. **Separates Feature Extraction from Downsampling:**\n",
    "   - Clear architectural separation: convolution learns features, pooling reduces dimensions\n",
    "   - This modularity makes the network easier to interpret and design\n",
    "   - Strided convolutions conflate these two operations\n",
    "\n",
    "**When Strided Convolution Might Be Better:**\n",
    "- When you need learnable downsampling (e.g., learning which information to preserve)\n",
    "- In very deep networks where every operation should be learnable\n",
    "- In architectures like fully convolutional networks where differentiability throughout is important\n",
    "\n",
    "**Conclusion:**\n",
    "For most traditional CNN architectures, max pooling is preferred because it provides spatial downsampling without adding parameters, improves translation invariance, and acts as a feature selector—all while being computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453594f",
   "metadata": {},
   "source": [
    "3. (3 points) **Consider a CNN composed of three convolutional layers, each with $3 \\times 3$ kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of $200 \\times 300$ pixels. What is the total number of parameters in the CNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa74fb5",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Given:\n",
    "- Input: RGB images of $200 \\times 300$ pixels (3 channels)\n",
    "- Layer 1: $3 \\times 3$ kernels, stride 2, \"same\" padding, 100 output feature maps\n",
    "- Layer 2: $3 \\times 3$ kernels, stride 2, \"same\" padding, 200 output feature maps\n",
    "- Layer 3: $3 \\times 3$ kernels, stride 2, \"same\" padding, 400 output feature maps\n",
    "\n",
    "**Calculating Parameters for Each Layer:**\n",
    "\n",
    "**Layer 1:**\n",
    "- Input channels: 3 (RGB)\n",
    "- Output feature maps: 100\n",
    "- Kernel size: $3 \\times 3$\n",
    "- Parameters per filter: $3 \\times 3 \\times 3 = 27$ weights\n",
    "- Bias per filter: 1\n",
    "- Total parameters: $100 \\times (27 + 1) = 100 \\times 28 = 2,800$\n",
    "\n",
    "**Layer 2:**\n",
    "- Input channels: 100 (from Layer 1)\n",
    "- Output feature maps: 200\n",
    "- Kernel size: $3 \\times 3$\n",
    "- Parameters per filter: $3 \\times 3 \\times 100 = 900$ weights\n",
    "- Bias per filter: 1\n",
    "- Total parameters: $200 \\times (900 + 1) = 200 \\times 901 = 180,200$\n",
    "\n",
    "**Layer 3:**\n",
    "- Input channels: 200 (from Layer 2)\n",
    "- Output feature maps: 400\n",
    "- Kernel size: $3 \\times 3$\n",
    "- Parameters per filter: $3 \\times 3 \\times 200 = 1,800$ weights\n",
    "- Bias per filter: 1\n",
    "- Total parameters: $400 \\times (1,800 + 1) = 400 \\times 1,801 = 720,400$\n",
    "\n",
    "**Total Number of Parameters:**\n",
    "$$\\text{Total} = 2,800 + 180,200 + 720,400 = \\boxed{903,400 \\text{ parameters}}$$\n",
    "\n",
    "**Formula for Convolutional Layer Parameters:**\n",
    "\n",
    "For a convolutional layer with:\n",
    "- $C_{in}$ input channels\n",
    "- $C_{out}$ output filters\n",
    "- Kernel size $K \\times K$\n",
    "\n",
    "$$\\text{Parameters} = C_{out} \\times (K \\times K \\times C_{in} + 1)$$\n",
    "\n",
    "where the \"+1\" accounts for the bias term per filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb1a5f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05482a2d",
   "metadata": {},
   "source": [
    "# On-Time (4 points) + Notebook PDF (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb96beb",
   "metadata": {},
   "source": [
    "Submit your Notebook PDF before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca49709",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Submit Your Solution\n",
    "\n",
    "Confirm that you've successfully completed the assignment.\n",
    "\n",
    "Along with the Notebook, include a PDF of the notebook with your solutions.\n",
    "\n",
    "```add``` and ```commit``` the final version of your work, and ```push``` your code to your GitHub repository.\n",
    "\n",
    "Submit the URL of your GitHub Repository as your assignment submission on Canvas.\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
